{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#适用版本paddlepaddle1.6.0\n",
    "\n",
    "import paddle \n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import FC, Conv2D, Embedding, Pool2D\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MovieLen(object):\n",
    "    def __init__(self, use_poster):\n",
    "        self.use_poster = use_poster\n",
    "        # 声明每个数据文件的路径\n",
    "        usr_info_path = \"./ml-1m/users.dat\"\n",
    "        if not use_poster:\n",
    "            rating_path = \"./ml-1m/ratings.dat\"\n",
    "        else:\n",
    "            rating_path = \"./ml-1m/new_rating.txt\"\n",
    "\n",
    "        movie_info_path = \"./ml-1m/movies.dat\"\n",
    "        self.poster_path = \"./ml-1m/posters/\"\n",
    "        # 得到电影数据\n",
    "        self.movie_info, self.movie_cat, self.movie_title = self.get_movie_info(movie_info_path)\n",
    "        # 记录电影的最大ID\n",
    "        self.max_mov_cat = np.max([self.movie_cat[k] for k in self.movie_cat])\n",
    "        self.max_mov_tit = np.max([self.movie_title[k] for k in self.movie_title])\n",
    "        self.max_mov_id = np.max(list(map(int, self.movie_info.keys())))\n",
    "        # 记录用户数据的最大ID\n",
    "        self.max_usr_id = 0\n",
    "        self.max_usr_age = 0\n",
    "        self.max_usr_job = 0\n",
    "        # 得到用户数据\n",
    "        self.usr_info = self.get_usr_info(usr_info_path)\n",
    "        # 得到评分数据\n",
    "        self.rating_info = self.get_rating_info(rating_path)\n",
    "        # 构建数据集 \n",
    "        self.dataset = self.get_dataset(usr_info=self.usr_info,\n",
    "                                        rating_info=self.rating_info,\n",
    "                                        movie_info=self.movie_info)\n",
    "        # 划分数据及，获得数据加载器\n",
    "        self.train_dataset = self.dataset[:int(len(self.dataset)*0.9)]\n",
    "        self.valid_dataset = self.dataset[int(len(self.dataset)*0.9):]\n",
    "        print(\"##Total dataset instances: \", len(self.dataset))\n",
    "        print(\"##MovieLens dataset information: \\nusr num: {}\\n\"\n",
    "              \"movies num: {}\".format(len(self.usr_info),len(self.movie_info)))\n",
    "    # 得到电影数据\n",
    "    def get_movie_info(self, path):\n",
    "        # 打开文件，编码方式选择ISO-8859-1，读取所有数据到data中 \n",
    "        with open(path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            data = f.readlines()\n",
    "        # 建立三个字典，分别用户存放电影所有信息，电影的名字信息、类别信息\n",
    "        movie_info, movie_titles, movie_cat = {}, {}, {}\n",
    "        # 对电影名字、类别中不同的单词计数\n",
    "        t_count, c_count = 1, 1\n",
    "\n",
    "        count_tit = {}\n",
    "        # 按行读取数据并处理\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            v_id = item[0]\n",
    "            v_title = item[1][:-7]\n",
    "            cats = item[2].split('|')\n",
    "            v_year = item[1][-5:-1]\n",
    "\n",
    "            titles = v_title.split()\n",
    "            # 统计电影名字的单词，并给每个单词一个序号，放在movie_titles中\n",
    "            for t in titles:\n",
    "                if t not in movie_titles:\n",
    "                    movie_titles[t] = t_count\n",
    "                    t_count += 1\n",
    "            # 统计电影类别单词，并给每个单词一个序号，放在movie_cat中\n",
    "            for cat in cats:\n",
    "                if cat not in movie_cat:\n",
    "                    movie_cat[cat] = c_count\n",
    "                    c_count += 1\n",
    "            # 补0使电影名称对应的列表长度为15\n",
    "            v_tit = [movie_titles[k] for k in titles]\n",
    "            while len(v_tit)<15:\n",
    "                v_tit.append(0)\n",
    "            # 补0使电影种类对应的列表长度为6\n",
    "            v_cat = [movie_cat[k] for k in cats]\n",
    "            while len(v_cat)<6:\n",
    "                v_cat.append(0)\n",
    "            # 保存电影数据到movie_info中\n",
    "            movie_info[v_id] = {'mov_id': int(v_id),\n",
    "                                'title': v_tit,\n",
    "                                'category': v_cat,\n",
    "                                'years': int(v_year)}\n",
    "        return movie_info, movie_cat, movie_titles\n",
    "\n",
    "    def get_usr_info(self, path):\n",
    "        # 性别转换函数，M-0， F-1\n",
    "        def gender2num(gender):\n",
    "            return 1 if gender == 'F' else 0\n",
    "\n",
    "        # 打开文件，读取所有行到data中\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "        # 建立用户信息的字典\n",
    "        use_info = {}\n",
    "\n",
    "        max_usr_id = 0\n",
    "        #按行索引数据\n",
    "        for item in data:\n",
    "            # 去除每一行中和数据无关的部分\n",
    "            item = item.strip().split(\"::\")\n",
    "            usr_id = item[0]\n",
    "            # 将字符数据转成数字并保存在字典中\n",
    "            use_info[usr_id] = {'usr_id': int(usr_id),\n",
    "                                'gender': gender2num(item[1]),\n",
    "                                'age': int(item[2]),\n",
    "                                'job': int(item[3])}\n",
    "            self.max_usr_id = max(self.max_usr_id, int(usr_id))\n",
    "            self.max_usr_age = max(self.max_usr_age, int(item[2]))\n",
    "            self.max_usr_job = max(self.max_usr_job, int(item[3]))\n",
    "        return use_info\n",
    "    # 得到评分数据\n",
    "    def get_rating_info(self, path):\n",
    "        # 读取文件里的数据\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "        # 将数据保存在字典中并返回\n",
    "        rating_info = {}\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            usr_id,movie_id,score = item[0],item[1],item[2]\n",
    "            if usr_id not in rating_info.keys():\n",
    "                rating_info[usr_id] = {movie_id:float(score)}\n",
    "            else:\n",
    "                rating_info[usr_id][movie_id] = float(score)\n",
    "        return rating_info\n",
    "    # 构建数据集\n",
    "    def get_dataset(self, usr_info, rating_info, movie_info):\n",
    "        trainset = []\n",
    "        for usr_id in rating_info.keys():\n",
    "            usr_ratings = rating_info[usr_id]\n",
    "            for movie_id in usr_ratings:\n",
    "                trainset.append({'usr_info': usr_info[usr_id],\n",
    "                                 'mov_info': movie_info[movie_id],\n",
    "                                 'scores': usr_ratings[movie_id]})\n",
    "        return trainset\n",
    "    \n",
    "    def load_data(self, dataset=None, mode='train'):\n",
    "        use_poster = False\n",
    "\n",
    "        # 定义数据迭代Batch大小\n",
    "        BATCHSIZE = 256\n",
    "\n",
    "        data_length = len(dataset)\n",
    "        index_list = list(range(data_length))\n",
    "        # 定义数据迭代加载器\n",
    "        def data_generator():\n",
    "            # 训练模式下，打乱训练数据\n",
    "            if mode == 'train':\n",
    "                random.shuffle(index_list)\n",
    "            # 声明每个特征的列表\n",
    "            usr_id_list,usr_gender_list,usr_age_list,usr_job_list = [], [], [], []\n",
    "            mov_id_list,mov_tit_list,mov_cat_list,mov_poster_list = [], [], [], []\n",
    "            score_list = []\n",
    "            # 索引遍历输入数据集\n",
    "            for idx, i in enumerate(index_list):\n",
    "                # 获得特征数据保存到对应特征列表中\n",
    "                usr_id_list.append(dataset[i]['usr_info']['usr_id'])\n",
    "                usr_gender_list.append(dataset[i]['usr_info']['gender'])\n",
    "                usr_age_list.append(dataset[i]['usr_info']['age'])\n",
    "                usr_job_list.append(dataset[i]['usr_info']['job'])\n",
    "\n",
    "                mov_id_list.append(dataset[i]['mov_info']['mov_id'])\n",
    "                mov_tit_list.append(dataset[i]['mov_info']['title'])\n",
    "                mov_cat_list.append(dataset[i]['mov_info']['category'])\n",
    "                mov_id = dataset[i]['mov_info']['mov_id']\n",
    "\n",
    "                if self.use_poster:\n",
    "                    # 不使用图像特征时，不读取图像数据，加快数据读取速度\n",
    "                    poster = Image.open(self.poster_path+'mov_id{}.jpg'.format(str(mov_id)))\n",
    "                    poster = poster.resize([64, 64])\n",
    "                    if len(poster.size) <= 2:\n",
    "                        poster = poster.convert(\"RGB\")\n",
    "\n",
    "                    mov_poster_list.append(np.array(poster))\n",
    "             \n",
    "                score_list.append(int(dataset[i]['scores']))\n",
    "                # 如果读取的数据量达到当前的batch大小，就返回当前批次\n",
    "                if len(usr_id_list)==BATCHSIZE:\n",
    "                    # 转换列表数据为数组形式，reshape到固定形状\n",
    "                    usr_id_arr = np.expand_dims(np.array(usr_id_list), axis=-1)\n",
    "                    usr_gender_arr = np.expand_dims(np.array(usr_gender_list), axis=-1)\n",
    "                    usr_age_arr = np.expand_dims(np.array(usr_age_list), axis=-1)\n",
    "                    usr_job_arr = np.expand_dims(np.array(usr_job_list), axis=-1)\n",
    "\n",
    "                    mov_id_arr = np.expand_dims(np.array(mov_id_list), axis=-1)\n",
    "                    mov_cat_arr = np.reshape(np.array(mov_cat_list), [BATCHSIZE, 1, 6, 1]).astype(np.int64)\n",
    "                    mov_tit_arr = np.reshape(np.array(mov_tit_list), [BATCHSIZE, 1, 15, 1]).astype(np.int64)\n",
    "\n",
    "#np.int64->np.float32\n",
    "                    if self.use_poster:\n",
    "                        mov_poster_arr = np.reshape(np.array(mov_poster_list)/127.5 - 1, [BATCHSIZE, 3, 64, 64]).astype(np.float32)\n",
    "                    else:\n",
    "                        mov_poster_arr = np.array([0.])\n",
    "\n",
    "                    scores_arr = np.reshape(np.array(score_list), [-1, 1]).astype(np.float32)\n",
    "                    \n",
    "                    # 放回当前批次数据\n",
    "                    yield [usr_id_arr, usr_gender_arr, usr_age_arr, usr_job_arr], \\\n",
    "                           [mov_id_arr, mov_cat_arr, mov_tit_arr, mov_poster_arr], scores_arr\n",
    "\n",
    "                    # 清空数据\n",
    "                    usr_id_list, usr_gender_list, usr_age_list, usr_job_list = [], [], [], []\n",
    "                    mov_id_list, mov_tit_list, mov_cat_list, score_list = [], [], [], []\n",
    "                    mov_poster_list = []\n",
    "        return data_generator\n",
    "\n",
    "class Model(dygraph.layers.Layer):\n",
    "    def __init__(self, name_scope, use_poster, use_mov_title, use_mov_cat, use_age_job):\n",
    "        super(Model, self).__init__(name_scope)\n",
    "        name = self.full_name()\n",
    "        \n",
    "        # 将传入的name信息和bool型参数添加到模型类中\n",
    "        self.use_mov_poster = use_poster\n",
    "        self.use_mov_title = use_mov_title\n",
    "        self.use_usr_age_job = use_age_job\n",
    "        self.use_mov_cat = use_mov_cat\n",
    "        \n",
    "        # 获取数据集的信息，并构建训练和验证集的数据迭代器\n",
    "        Dataset = MovieLen(self.use_mov_poster)\n",
    "        self.Dataset = Dataset\n",
    "        self.trainset = self.Dataset.train_dataset\n",
    "        self.valset = self.Dataset.valid_dataset\n",
    "        self.train_loader = self.Dataset.load_data(dataset=self.trainset, mode='train')\n",
    "        self.valid_loader = self.Dataset.load_data(dataset=self.valset, mode='valid')\n",
    "\n",
    "        \"\"\" define network layer for embedding usr info \"\"\"\n",
    "        USR_ID_NUM = Dataset.max_usr_id + 1\n",
    "        # 对用户ID做映射，并紧接着一个FC层\n",
    "        self.usr_emb = Embedding(name, [USR_ID_NUM, 32], is_sparse=False)\n",
    "        self.usr_fc = FC(name, size=32)\n",
    "        \n",
    "        # 对用户性别信息做映射，并紧接着一个FC层\n",
    "        USR_GENDER_DICT_SIZE = 2\n",
    "        self.usr_gender_emb = Embedding(name, [USR_GENDER_DICT_SIZE, 16])\n",
    "        self.usr_gender_fc = FC(name, 16)\n",
    "        \n",
    "        # 对用户年龄信息做映射，并紧接着一个FC层\n",
    "        USR_AGE_DICT_SIZE = Dataset.max_usr_age + 1\n",
    "        self.usr_age_emb = Embedding(name, [USR_AGE_DICT_SIZE, 16])\n",
    "        self.usr_age_fc = FC(name, 16)\n",
    "        \n",
    "        # 对用户职业信息做映射，并紧接着一个FC层\n",
    "        USR_JOB_DICT_SIZE = Dataset.max_usr_job + 1\n",
    "        self.usr_job_emb = Embedding(name, [USR_JOB_DICT_SIZE, 16])\n",
    "        self.usr_job_fc = FC(name, 16)\n",
    "        \n",
    "        # 新建一个FC层，用于整合用户数据信息\n",
    "        self.usr_combined = FC(name, 200, act='tanh')\n",
    "        \n",
    "        \"\"\" define network layer for embedding usr info \"\"\"\n",
    "        # 对电影ID信息做映射，并紧接着一个FC层\n",
    "        MOV_DICT_SIZE = Dataset.max_mov_id + 1\n",
    "        self.mov_emb = Embedding(name, [MOV_DICT_SIZE, 32])\n",
    "        self.mov_fc = FC(name, 32)\n",
    "        \n",
    "        # 对电影类别做映射\n",
    "        CATEGORY_DICT_SIZE = len(Dataset.movie_cat) + 1\n",
    "        self.mov_cat_emb = Embedding(name, [CATEGORY_DICT_SIZE, 32], is_sparse=False)\n",
    "        self.mov_cat_fc = FC(name, 32)\n",
    "        \n",
    "        # 对电影名称做映射\n",
    "        MOV_TITLE_DICT_SIZE = len(Dataset.movie_title) + 1\n",
    "        self.mov_title_emb = Embedding(name, [MOV_TITLE_DICT_SIZE, 32], is_sparse=False)\n",
    "        self.mov_title_conv = Conv2D(name, 1, filter_size=(3, 1), stride=(2,1), padding=0, act='relu')\n",
    "        self.mov_title_conv2 = Conv2D(name, 1, filter_size=(3, 1), stride=1, padding=0, act='relu')\n",
    "\n",
    "        #对电影海报做特征\n",
    "        self.mov_poster_conv= Conv2D(name,50,filter_size=10,stride=(1,1),padding=0,act='relu' )\n",
    "        self.mov_poster_pool= Pool2D(name,pool_size=5,pool_type='max', pool_stride=1,pool_padding=0)\n",
    "        self.mov_poster_conv2=Conv2D(name,50,filter_size=3,stride=1,padding=0,act='relu' )\n",
    "        self.mov_poster_pool2= Pool2D(name,pool_size=2,pool_type='max', pool_stride=1,pool_padding=0)\n",
    "        self.mov_poster_fc = FC(name, 32)\n",
    "        # 新建一个FC层，用于整合电影特征\n",
    "        self.mov_concat_embed = FC(name, size=200, act='tanh')\n",
    "        \n",
    "    # 定义计算用户特征的前向运算过程\n",
    "    def get_usr_feat(self, usr_var):\n",
    "        \"\"\" get usr features\"\"\"\n",
    "        # 获取到用户数据\n",
    "        usr_id, usr_gender, usr_age, usr_job = usr_var\n",
    "        # 将用户的ID数据经过embedding和FC计算，得到的特征保存在feats_collect中\n",
    "        feats_collect = []\n",
    "        usr_id = self.usr_emb(usr_id)\n",
    "        usr_id = self.usr_fc(usr_id)\n",
    "        usr_id = fluid.layers.relu(usr_id)\n",
    "        feats_collect.append(usr_id)\n",
    "        \n",
    "        # 计算用户的性别特征，并保存在feats_collect中\n",
    "        usr_gender = self.usr_gender_emb(usr_gender)\n",
    "        usr_gender = self.usr_gender_fc(usr_gender)\n",
    "        usr_gender = fluid.layers.relu(usr_gender)\n",
    "        feats_collect.append(usr_gender)\n",
    "        # 选择是否使用用户的年龄-职业特征\n",
    "        if self.use_usr_age_job:\n",
    "            # 计算用户的年龄特征，并保存在feats_collect中\n",
    "            usr_age = self.usr_age_emb(usr_age)\n",
    "            usr_age = self.usr_age_fc(usr_age)\n",
    "            usr_age = fluid.layers.relu(usr_age)\n",
    "            feats_collect.append(usr_age)\n",
    "            # 计算用户的职业特征，并保存在feats_collect中\n",
    "            usr_job = self.usr_job_emb(usr_job)\n",
    "            usr_job = self.usr_job_fc(usr_job)\n",
    "            usr_job = fluid.layers.relu(usr_job)\n",
    "            feats_collect.append(usr_job)\n",
    "        \n",
    "        # 将用户的特征级联，并通过FC层得到最终的用户特征\n",
    "        usr_feat = fluid.layers.concat(feats_collect, axis=1)\n",
    "        usr_feat = self.usr_combined(usr_feat)\n",
    "        return usr_feat\n",
    "\n",
    "        # 定义电影特征的前向计算过程\n",
    "    def get_mov_feat(self, mov_var):\n",
    "        \"\"\" get movie features\"\"\"\n",
    "        # 获得电影数据\n",
    "        mov_id, mov_cat, mov_title, mov_poster = mov_var\n",
    "        feats_collect = []\n",
    "        # 获得batchsize的大小\n",
    "        batch_size = mov_id.shape[0]\n",
    "        # 计算电影ID的特征，并存在feats_collect中\n",
    "        mov_id = self.mov_emb(mov_id)\n",
    "        mov_id = self.mov_fc(mov_id)\n",
    "        mov_id = fluid.layers.relu(mov_id)\n",
    "        feats_collect.append(mov_id)\n",
    "        \n",
    "        # 如果使用电影的种类数据，计算电影种类特征的映射\n",
    "        if self.use_mov_cat:\n",
    "            # 计算电影种类的特征映射，对多个种类的特征求和得到最终特征\n",
    "            mov_cat = self.mov_cat_emb(mov_cat)\n",
    "            mov_cat = fluid.layers.reduce_sum(mov_cat, dim=1, keep_dim=False)\n",
    "\n",
    "            mov_cat = self.mov_cat_fc(mov_cat)\n",
    "            feats_collect.append(mov_cat)\n",
    "\n",
    "        if self.use_mov_title:\n",
    "            # 计算电影名字的特征映射，对特征映射使用卷积计算最终的特征\n",
    "            mov_title = self.mov_title_emb(mov_title)\n",
    "            mov_title = self.mov_title_conv2(self.mov_title_conv(mov_title))\n",
    "            mov_title = fluid.layers.reduce_sum(mov_title, dim=2, keep_dim=False)\n",
    "            mov_title = fluid.layers.relu(mov_title)\n",
    "            mov_title = fluid.layers.reshape(mov_title, [batch_size, -1])\n",
    "            feats_collect.append(mov_title)\n",
    "\n",
    "        if self.use_mov_poster:\n",
    "\n",
    "            mov_poster=self.mov_poster_conv(mov_poster)\n",
    "            mov_poster=self.mov_poster_pool(mov_poster)\n",
    "            mov_poster=self.mov_poster_conv2(mov_poster)\n",
    "            mov_poster=self.mov_poster_pool2(mov_poster)\n",
    "            mov_poster = self.mov_poster_fc(mov_poster)\n",
    "            feats_collect.append(mov_poster)\n",
    "\n",
    "        # 使用一个全连接层，整合所有电影特征，映射为一个200维的特征向量\n",
    "        mov_feat = fluid.layers.concat(feats_collect, axis=1)\n",
    "        mov_feat = self.mov_concat_embed(mov_feat)\n",
    "        return mov_feat\n",
    "    \n",
    "    # 定义个性化推荐算法的前向计算\n",
    "    def forward(self, usr_var, mov_var):\n",
    "        # 计算用户特征和电影特征\n",
    "        usr_feat = self.get_usr_feat(usr_var)\n",
    "        mov_feat = self.get_mov_feat(mov_var)\n",
    "        # 根据计算的特征计算相似度\n",
    "        res = fluid.layers.cos_sim(usr_feat, mov_feat)\n",
    "        # 将相似度扩大范围到和电影评分相同数据范围\n",
    "        res = fluid.layers.scale(res, scale=5)\n",
    "        return usr_feat, mov_feat, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # 配置训练参数\n",
    "    use_gpu = False\n",
    "    lr = 0.01\n",
    "    Epoches = 10\n",
    "\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 启动训练\n",
    "        model.train()\n",
    "        # 获得数据读取器\n",
    "        data_loader = model.train_loader\n",
    "        # 使用adam优化器，学习率使用0.01\n",
    "        opt = fluid.optimizer.Adam(learning_rate=lr)\n",
    "        \n",
    "        for epoch in range(0, Epoches):\n",
    "            for idx, data in enumerate(data_loader()):\n",
    "                # 获得数据，并转为动态图格式\n",
    "                usr, mov, score = data\n",
    "                usr_v = [dygraph.to_variable(np.int64(var)) for var in usr]\n",
    "                mov_v = [dygraph.to_variable(np.int64(var)) for var in mov[:-1]]\n",
    "                mov_v.append(dygraph.to_variable(np.float32(mov[-1])))\n",
    "                \n",
    "                scores_label = dygraph.to_variable(score)\n",
    "                # 计算出算法的前向计算结果\n",
    "                _, _, scores_predict = model(usr_v, mov_v)\n",
    "                # 计算loss\n",
    "#                print(scores_predict.dtype,scores_label.dtype)\n",
    "                loss = fluid.layers.square_error_cost(scores_predict, scores_label)\n",
    "                avg_loss = fluid.layers.mean(loss)\n",
    "                if idx % 500 == 0:\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, idx, avg_loss.numpy()))\n",
    "                    \n",
    "                # 损失函数下降，并清除梯度\n",
    "                avg_loss.backward()\n",
    "                opt.minimize(avg_loss)\n",
    "                model.clear_gradients()\n",
    "            # 每个epoch 保存一次模型\n",
    "            fluid.save_dygraph(model.state_dict(), './checkpoint/epoch'+str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, params_file_path):\n",
    "    use_gpu = False\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "\n",
    "        model_state_dict, _ = fluid.load_dygraph(params_file_path)\n",
    "        model.load_dict(model_state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        acc_set = []\n",
    "        avg_loss_set = []\n",
    "        for idx, data in enumerate(model.valid_loader()):\n",
    "            usr, mov, score_label = data\n",
    "            usr_v = [dygraph.to_variable(np.int64(var)) for var in usr]\n",
    "            mov_v = [dygraph.to_variable(np.int64(var)) for var in mov[:-1]]\n",
    "            mov_v.append(dygraph.to_variable(np.float32(mov[-1])))\n",
    "\n",
    "            _, _, scores_predict = model(usr_v, mov_v)\n",
    "\n",
    "            pred_scores = scores_predict.numpy()\n",
    "            \n",
    "            avg_loss_set.append(np.mean(np.abs(pred_scores - score_label)))\n",
    "\n",
    "            diff = np.abs(pred_scores - score_label)\n",
    "            diff[diff>0.5] = 1\n",
    "            acc = 1 - np.mean(diff)\n",
    "            acc_set.append(acc)\n",
    "        return np.mean(acc_set), np.mean(avg_loss_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Total dataset instances:  1000209\n",
      "##MovieLens dataset information: \n",
      "usr num: 6040\n",
      "movies num: 3883\n",
      "epoch: 0, batch_id: 0, loss is: [15.340841]\n",
      "epoch: 0, batch_id: 500, loss is: [1.0508535]\n",
      "epoch: 0, batch_id: 1000, loss is: [1.1642591]\n",
      "epoch: 0, batch_id: 1500, loss is: [1.1351136]\n",
      "epoch: 0, batch_id: 2000, loss is: [0.96807575]\n",
      "epoch: 0, batch_id: 2500, loss is: [0.9790999]\n",
      "epoch: 0, batch_id: 3000, loss is: [1.0027442]\n",
      "epoch: 0, batch_id: 3500, loss is: [0.9665835]\n",
      "epoch: 1, batch_id: 0, loss is: [0.9519994]\n",
      "epoch: 1, batch_id: 500, loss is: [1.0032053]\n",
      "epoch: 1, batch_id: 1000, loss is: [1.039664]\n",
      "epoch: 1, batch_id: 1500, loss is: [1.216085]\n",
      "epoch: 1, batch_id: 2000, loss is: [0.95938915]\n",
      "epoch: 1, batch_id: 2500, loss is: [1.0065184]\n",
      "epoch: 1, batch_id: 3000, loss is: [1.0903273]\n",
      "epoch: 1, batch_id: 3500, loss is: [0.92516017]\n",
      "epoch: 2, batch_id: 0, loss is: [0.9524243]\n",
      "epoch: 2, batch_id: 500, loss is: [1.032253]\n",
      "epoch: 2, batch_id: 1000, loss is: [0.92622983]\n",
      "epoch: 2, batch_id: 1500, loss is: [0.91010654]\n",
      "epoch: 2, batch_id: 2000, loss is: [0.9743533]\n",
      "epoch: 2, batch_id: 2500, loss is: [1.0518062]\n",
      "epoch: 2, batch_id: 3000, loss is: [0.9883028]\n",
      "epoch: 2, batch_id: 3500, loss is: [0.9638851]\n",
      "epoch: 3, batch_id: 0, loss is: [1.0386624]\n",
      "epoch: 3, batch_id: 500, loss is: [0.983703]\n",
      "epoch: 3, batch_id: 1000, loss is: [0.89738226]\n",
      "epoch: 3, batch_id: 1500, loss is: [0.8810539]\n",
      "epoch: 3, batch_id: 2000, loss is: [0.8817059]\n",
      "epoch: 3, batch_id: 2500, loss is: [0.9230603]\n",
      "epoch: 3, batch_id: 3000, loss is: [0.7325852]\n",
      "epoch: 3, batch_id: 3500, loss is: [0.86536777]\n",
      "epoch: 4, batch_id: 0, loss is: [0.83725053]\n",
      "epoch: 4, batch_id: 500, loss is: [0.8729471]\n",
      "epoch: 4, batch_id: 1000, loss is: [0.83149314]\n",
      "epoch: 4, batch_id: 1500, loss is: [0.9316689]\n",
      "epoch: 4, batch_id: 2000, loss is: [0.9278513]\n",
      "epoch: 4, batch_id: 2500, loss is: [0.89104676]\n",
      "epoch: 4, batch_id: 3000, loss is: [0.8676048]\n",
      "epoch: 4, batch_id: 3500, loss is: [0.8292604]\n",
      "epoch: 5, batch_id: 0, loss is: [0.91363585]\n",
      "epoch: 5, batch_id: 500, loss is: [0.90156144]\n",
      "epoch: 5, batch_id: 1000, loss is: [0.8558292]\n",
      "epoch: 5, batch_id: 1500, loss is: [0.9742195]\n",
      "epoch: 5, batch_id: 2000, loss is: [0.91548073]\n",
      "epoch: 5, batch_id: 2500, loss is: [0.8374849]\n",
      "epoch: 5, batch_id: 3000, loss is: [0.8447553]\n",
      "epoch: 5, batch_id: 3500, loss is: [1.0357726]\n",
      "epoch: 6, batch_id: 0, loss is: [0.8120361]\n",
      "epoch: 6, batch_id: 500, loss is: [0.9034709]\n",
      "epoch: 6, batch_id: 1000, loss is: [0.8145592]\n",
      "epoch: 6, batch_id: 1500, loss is: [0.8845079]\n",
      "epoch: 6, batch_id: 2000, loss is: [0.89512455]\n",
      "epoch: 6, batch_id: 2500, loss is: [0.9226932]\n",
      "epoch: 6, batch_id: 3000, loss is: [0.72097003]\n",
      "epoch: 6, batch_id: 3500, loss is: [0.8705146]\n",
      "epoch: 7, batch_id: 0, loss is: [1.0191655]\n",
      "epoch: 7, batch_id: 500, loss is: [0.9075723]\n",
      "epoch: 7, batch_id: 1000, loss is: [0.91528016]\n",
      "epoch: 7, batch_id: 1500, loss is: [0.9143941]\n",
      "epoch: 7, batch_id: 2000, loss is: [1.0957196]\n",
      "epoch: 7, batch_id: 2500, loss is: [0.95237476]\n",
      "epoch: 7, batch_id: 3000, loss is: [0.7969247]\n",
      "epoch: 7, batch_id: 3500, loss is: [0.80867386]\n",
      "epoch: 8, batch_id: 0, loss is: [0.99639815]\n",
      "epoch: 8, batch_id: 500, loss is: [0.8323028]\n",
      "epoch: 8, batch_id: 1000, loss is: [0.83791584]\n",
      "epoch: 8, batch_id: 1500, loss is: [0.9994086]\n",
      "epoch: 8, batch_id: 2000, loss is: [0.93205214]\n",
      "epoch: 8, batch_id: 2500, loss is: [0.9345081]\n",
      "epoch: 8, batch_id: 3000, loss is: [0.77758884]\n",
      "epoch: 8, batch_id: 3500, loss is: [0.9263575]\n",
      "epoch: 9, batch_id: 0, loss is: [0.845811]\n",
      "epoch: 9, batch_id: 500, loss is: [0.8920934]\n",
      "epoch: 9, batch_id: 1000, loss is: [0.9230171]\n",
      "epoch: 9, batch_id: 1500, loss is: [0.84288406]\n",
      "epoch: 9, batch_id: 2000, loss is: [0.8129151]\n",
      "epoch: 9, batch_id: 2500, loss is: [0.8477969]\n",
      "epoch: 9, batch_id: 3000, loss is: [0.7562385]\n",
      "epoch: 9, batch_id: 3500, loss is: [0.9651629]\n"
     ]
    }
   ],
   "source": [
    "# 启动训练,使用了除海报外的所有特征\n",
    "with dygraph.guard():\n",
    "    use_poster, use_mov_title, use_mov_cat, use_age_job = False, True, True, True\n",
    "    model = Model('Recommend', use_poster, use_mov_title, use_mov_cat, use_age_job)\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.26040655542642643 MAE: 0.85908407\n",
      "ACC: 0.2693785063731365 MAE: 0.84108776\n",
      "ACC: 0.2783773402372996 MAE: 0.83288324\n",
      "ACC: 0.27654058077396493 MAE: 0.82881063\n",
      "ACC: 0.2784864398149344 MAE: 0.82926965\n",
      "ACC: 0.2767193344923166 MAE: 0.82750934\n",
      "ACC: 0.28125518262386323 MAE: 0.8246155\n",
      "ACC: 0.2790882761661823 MAE: 0.8232499\n",
      "ACC: 0.2773896784354479 MAE: 0.8260784\n",
      "ACC: 0.2735202075579228 MAE: 0.8306713\n"
     ]
    }
   ],
   "source": [
    "param_path = \"./checkpoint/epoch\"\n",
    "for i in range(10):\n",
    "    acc, mae = evaluation(model, param_path+str(i))\n",
    "    print(\"ACC:\", acc, \"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Total dataset instances:  382499\n",
      "##MovieLens dataset information: \n",
      "usr num: 6040\n",
      "movies num: 3883\n",
      "epoch: 0, batch_id: 0, loss is: [17.017052]\n"
     ]
    }
   ],
   "source": [
    "with dygraph.guard():\n",
    "    use_poster, use_mov_title, use_mov_cat, use_age_job = True, True, True, True\n",
    "    model = Model('Recommend', use_poster, use_mov_title, use_mov_cat, use_age_job)\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n",
      "usr / mov features saved!!!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# 加载第三方库Pickle，用来保存Python数据到本地\n",
    "import pickle\n",
    "# 定义特征保存函数\n",
    "def get_usr_mov_features(model, params_file_path, poster_path):\n",
    "    use_gpu = False\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "    usr_pkl = {}\n",
    "    mov_pkl = {}\n",
    "    \n",
    "    # 定义将list中每个元素转成variable的函数\n",
    "    def list2variable(inputs, shape):\n",
    "        inputs = np.reshape(np.array(inputs).astype(np.int64), shape)\n",
    "        return fluid.dygraph.to_variable(inputs)\n",
    "    \n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 加载模型参数到模型中，设置为验证模式eval（）\n",
    "        model_state_dict, _ = fluid.load_dygraph(params_file_path)\n",
    "        model.load_dict(model_state_dict)\n",
    "        model.eval()\n",
    "        # 获得整个数据集的数据\n",
    "        dataset = model.Dataset.dataset\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            # 获得用户数据，电影数据，评分数据  \n",
    "            # 本案例只转换所有在样本中出现过的user和movie，实际中可以使用业务系统中的全量数据\n",
    "            usr_info, mov_info, score = dataset[i]['usr_info'], dataset[i]['mov_info'],dataset[i]['scores']\n",
    "            usrid = str(usr_info['usr_id'])\n",
    "            movid = str(mov_info['mov_id'])\n",
    "\n",
    "            # 获得用户数据，计算得到用户特征，保存在usr_pkl字典中\n",
    "            if usrid not in usr_pkl.keys():\n",
    "                usr_id_v = list2variable(usr_info['usr_id'], [1, 1])\n",
    "                usr_age_v = list2variable(usr_info['age'], [1, 1])\n",
    "                usr_gender_v = list2variable(usr_info['gender'], [1, 1])\n",
    "                usr_job_v = list2variable(usr_info['job'], [1, 1])\n",
    "\n",
    "                usr_in = [usr_id_v, usr_gender_v, usr_age_v, usr_job_v]\n",
    "                usr_feat = model.get_usr_feat(usr_in)\n",
    "\n",
    "                usr_pkl[usrid] = usr_feat.numpy()\n",
    "            \n",
    "            # 获得电影数据，计算得到电影特征，保存在mov_pkl字典中\n",
    "            if movid not in mov_pkl.keys():\n",
    "                mov_id_v = list2variable(mov_info['mov_id'], [1, 1])\n",
    "                mov_tit_v = list2variable(mov_info['title'], [1, 1, 15, 1])\n",
    "                mov_cat_v = list2variable(mov_info['category'], [1, 1, 6, 1])\n",
    "\n",
    "                mov_in = [mov_id_v, mov_cat_v, mov_tit_v, None]\n",
    "                mov_feat = model.get_mov_feat(mov_in)\n",
    "\n",
    "                mov_pkl[movid] = mov_feat.numpy()\n",
    "    \n",
    "    print(len(mov_pkl.keys()))\n",
    "    # 保存特征到本地\n",
    "    pickle.dump(usr_pkl, open('./usr_feat.pkl', 'wb'))\n",
    "    pickle.dump(mov_pkl, open('./mov_feat.pkl', 'wb'))\n",
    "    print(\"usr / mov features saved!!!\")\n",
    "\n",
    "        \n",
    "param_path = \"./checkpoint/epoch7\"\n",
    "poster_path = \"./ml-1m/posters/\"\n",
    "get_usr_mov_features(model, param_path, poster_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前的用户是：\n",
      "usr_id: 2\n",
      "推荐可能喜欢的电影是：\n",
      "mov_id: 913 ['913', 'Maltese Falcon, The (1941)', 'Film-Noir|Mystery']\n",
      "mov_id: 1198 ['1198', 'Raiders of the Lost Ark (1981)', 'Action|Adventure']\n",
      "mov_id: 1898 ['1898', 'Land Girls, The (1998)', 'Drama|War']\n",
      "mov_id: 260 ['260', 'Star Wars: Episode IV - A New Hope (1977)', 'Action|Adventure|Fantasy|Sci-Fi']\n",
      "mov_id: 50 ['50', 'Usual Suspects, The (1995)', 'Crime|Thriller']\n"
     ]
    }
   ],
   "source": [
    "#第一个推荐系统，即课程中的那个\n",
    "# 定义根据用户兴趣推荐电影\n",
    "def recommend_mov_for_usr(usr_id, top_k, pick_num, usr_feat_dir, mov_feat_dir, mov_info_path):\n",
    "    assert pick_num <= top_k\n",
    "    # 读取电影和用户的特征\n",
    "    usr_feats = pickle.load(open(usr_feat_dir, 'rb'))\n",
    "    mov_feats = pickle.load(open(mov_feat_dir, 'rb'))\n",
    "    usr_feat = usr_feats[str(usr_id)]\n",
    "\n",
    "    cos_sims = []\n",
    "\n",
    "    with dygraph.guard():\n",
    "        # 索引电影特征，计算和输入用户ID的特征的相似度\n",
    "        for idx, key in enumerate(mov_feats.keys()):\n",
    "            mov_feat = mov_feats[key]\n",
    "            usr_feat = dygraph.to_variable(usr_feat)\n",
    "            mov_feat = dygraph.to_variable(mov_feat)\n",
    "            sim = fluid.layers.cos_sim(usr_feat, mov_feat)\n",
    "            cos_sims.append(sim.numpy()[0][0])\n",
    "    # 对相似度排序\n",
    "    index = np.argsort(cos_sims)[-top_k:]\n",
    "\n",
    "    mov_info = {}\n",
    "    # 读取电影文件里的数据，根据电影ID索引到电影信息\n",
    "    with open(mov_info_path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            mov_info[str(item[0])] = item\n",
    "            \n",
    "    print(\"当前的用户是：\")\n",
    "    print(\"usr_id:\", usr_id)\n",
    "    print(\"推荐可能喜欢的电影是：\")\n",
    "    res = []\n",
    "    \n",
    "    # 加入随机选择因素，确保每次推荐的都不一样\n",
    "    while len(res) < pick_num:\n",
    "        val = np.random.choice(len(index), 1)[0]\n",
    "        idx = index[val]\n",
    "        mov_id = list(mov_feats.keys())[idx]\n",
    "        if mov_id not in res:\n",
    "            res.append(mov_id)\n",
    "\n",
    "    for id in res:\n",
    "        print(\"mov_id:\", id, mov_info[str(id)])\n",
    "\n",
    "movie_data_path = \"./ml-1m/movies.dat\"\n",
    "top_k, pick_num = 10, 5\n",
    "\n",
    "recommend_mov_for_usr(2, top_k, pick_num, 'usr_feat.pkl', 'mov_feat.pkl', movie_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要推荐的用户是：user:['1', 'F', '1', '10', '48067']\n",
      "与其相似的用户分别是：user:['1411', 'M', '35', '1', '08107']\n",
      "与其相似的用户分别是：user:['1936', 'M', '35', '0', '90026']\n",
      "与其相似的用户分别是：user:['1996', 'M', '35', '0', '85621']\n",
      "与其相似的用户分别是：user:['2418', 'F', '1', '10', '06074']\n",
      "与其相似的用户分别是：user:['896', 'M', '18', '15', '94015']\n",
      "要推荐的电影为 mov_id: 3173 ['3173', 'Any Given Sunday (1999)', 'Drama']\n",
      "要推荐的电影为 mov_id: 3743 ['3743', 'Boys and Girls (2000)', 'Comedy|Romance']\n",
      "要推荐的电影为 mov_id: 306 ['306', 'Three Colors: Red (1994)', 'Drama']\n",
      "要推荐的电影为 mov_id: 1240 ['1240', 'Terminator, The (1984)', 'Action|Sci-Fi|Thriller']\n",
      "要推荐的电影为 mov_id: 1234 ['1234', 'Sting, The (1973)', 'Comedy|Crime']\n"
     ]
    }
   ],
   "source": [
    "#第二个推荐系统，根据相似用户推荐电影（user-based）\n",
    "\n",
    "# 给定一个用户ID，找到评分最高的topk个电影\n",
    "def gen_movie_with_usr(usr_a,topk):\n",
    "    usr_a=usr_a\n",
    "    rating_path = \"./ml-1m/ratings.dat\"\n",
    "    # 打开文件，ratings_data\n",
    "    with open(rating_path, 'r') as f:\n",
    "        ratings_data = f.readlines()\n",
    "    \n",
    "    usr_rating_info = {}\n",
    "    for item in ratings_data:\n",
    "        item = item.strip().split(\"::\")\n",
    "        # 处理每行数据，分别得到用户ID，电影ID，和评分\n",
    "        usr_id,movie_id,score = item[0],item[1],item[2]\n",
    "        if str(usr_id) == str(usr_a):\n",
    "            usr_rating_info[movie_id] = float(score)\n",
    "\n",
    "    # 获得评分过的电影ID\n",
    "    movie_ids = list(usr_rating_info.keys())\n",
    " #   print(\"ID为 {} 的用户，评分过的电影数量是: \".format(usr_a), len(movie_ids))\n",
    "\n",
    "    ratings_topk = sorted(usr_rating_info.items(), key=lambda item:item[1])[-topk:]    \n",
    "    return ratings_topk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#根据相似用户推荐电影\n",
    "\n",
    "import pickle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "import numpy as np\n",
    "\n",
    "#以下参数的top_k是参考最为相似的用户个数，在最为相似的几个用户中获得相应看过的电影抽取pick_num部电影推荐给用户\n",
    "def recommend_mov_user_based(usr_id, top_k, pick_num, usr_feat_dir, mov_feat_dir,usr_info_path,mov_info_path):\n",
    "\n",
    "    \n",
    "    movie_data_path = mov_info_path\n",
    "    mov_info = {}\n",
    "    # 打开电影数据文件，根据电影ID索引到电影信息\n",
    "    with open(movie_data_path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            mov_info[str(item[0])] = item\n",
    "\n",
    "    usr_file = usr_info_path\n",
    "    usr_info = {}\n",
    "    # 打开文件，读取所有行到data中\n",
    "    with open(usr_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            usr_info[str(item[0])] = item\n",
    "\n",
    "    \n",
    "   \n",
    "   \n",
    "    # 根据用户ID获得该用户的特征\n",
    "    usr_ID = usr_id\n",
    "    # 读取保存的用户特征\n",
    "    usr_feat_dir = usr_feat_dir\n",
    "    usr_feats = pickle.load(open(usr_feat_dir, 'rb'))\n",
    "    # 根据用户ID索引到该用户的特征\n",
    "    usr_ID_feat = usr_feats[str(usr_ID)]\n",
    "\n",
    "    # 记录计算的相似度\n",
    "    cos_sims = []\n",
    "    # 记录下与用户特征计算相似的电影顺序\n",
    "\n",
    "    with dygraph.guard():\n",
    "        # 索引电影特征，计算和输入用户ID的特征的相似度\n",
    "        for idx, key in enumerate(usr_feats.keys()):\n",
    "            usr_feat = usr_feats[key]\n",
    "            usr_query_feat = dygraph.to_variable(usr_ID_feat)\n",
    "            usr_feat = dygraph.to_variable(usr_feat)\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            sim = fluid.layers.cos_sim(usr_query_feat,usr_feat)\n",
    "            # 打印特征和相似度的形状\n",
    "#            if idx==0:\n",
    "#                print(\"用户特征形状：{}, 用户特征形状：{}, 相似度结果形状：{}，相似度结果：{}\".format(usr_query_feat.shape, usr_feat0.shape, sim.numpy().shape, sim.numpy()))\n",
    "            # 从形状为（1，1）的相似度sim中获得相似度值sim.numpy()[0][0]，并添加到相似度列表cos_sims中\n",
    "            cos_sims.append(sim.numpy()[0][0])\n",
    "\n",
    "\n",
    "    # 3. 对相似度排序，获得最大相似度在cos_sims中的位置\n",
    "    index = np.argsort(cos_sims)\n",
    "    # 打印相似度最大的前topk个位置\n",
    "    topk = top_k+1\n",
    "#   print(\"相似度最大的前{}个索引是{}\\n对应的相似度是：{}\\n\".format(topk-1, index[-topk:-1], [cos_sims[k] for k in index[-topk:-1]]))\n",
    "    print('要推荐的用户是：user:{}'.format(usr_info[list(usr_feats.keys())[usr_ID-1]]))\n",
    "    recommend_mov=[]\n",
    "    for i in index[-topk:-1]:           \n",
    "        print(\"与其相似的用户分别是：user:{}\".format(usr_info[list(usr_feats.keys())[i]]))\n",
    "\n",
    "\n",
    "        ratings_topk=gen_movie_with_usr(usr_info[list(usr_feats.keys())[i]][0],5)\n",
    "        for k, score in ratings_topk:\n",
    " #           print(\"电影ID: {}，评分是: {}, 电影信息: {}\".format(k, score, movie_info[k]))\n",
    "            recommend_mov.append(k)\n",
    "#用于推荐的电影\n",
    "    recommend_mov=set(recommend_mov)\n",
    "    recommend_mov=list(recommend_mov)\n",
    "    res = []\n",
    "# 加入随机选择因素，确保每次推荐的结果稍有差别\n",
    "    while len(res) < pick_num:\n",
    "        mov_id = np.random.choice(len(recommend_mov), 1)[0]\n",
    "        if recommend_mov[mov_id] not in res:\n",
    "            res.append(recommend_mov[mov_id])\n",
    "\n",
    "    for id in res:\n",
    "        print(\"要推荐的电影为 mov_id:\", id, mov_info[str(id)])\n",
    "\n",
    "        \n",
    "usr_feat_dir='usr_feat.pkl'\n",
    "mov_feat_dir='mov_feat.pkl'\n",
    "mov_info_path=\"./ml-1m/movies.dat\"\n",
    "usr_info_path=\"./ml-1m/users.dat\"\n",
    "recommend_mov_user_based(1, 5, 5, usr_feat_dir, mov_feat_dir,usr_info_path, mov_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户1评分高的电影是：['1029', 'Dumbo (1941)', \"Animation|Children's|Musical\"]\n",
      "与其相似的电影分别是：['914', 'My Fair Lady (1964)', 'Musical|Romance']\n",
      "与其相似的电影分别是：['244', 'Gumby: The Movie (1995)', \"Animation|Children's\"]\n",
      "与其相似的电影分别是：['2294', 'Antz (1998)', \"Animation|Children's\"]\n",
      "与其相似的电影分别是：['3606', 'On the Town (1949)', 'Musical']\n",
      "与其相似的电影分别是：['2139', 'Secret of NIMH, The (1982)', \"Animation|Children's\"]\n"
     ]
    }
   ],
   "source": [
    "#第三个推荐系统，根据相似电影推荐电影（item-based）\n",
    "#tok_k是用户曾经看过的并且评价最高的电影部数，从中选出一部将与该部相似的pick_num部电影推荐给用户\n",
    "def recommend_mov_item_based(usr_id, top_k, pick_num, usr_feat_dir, mov_feat_dir, usr_info_path,mov_info_path):\n",
    "    \n",
    "    movie_data_path = mov_info_path\n",
    "    mov_info = {}\n",
    "    # 打开电影数据文件，根据电影ID索引到电影信息\n",
    "    with open(movie_data_path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            mov_info[str(item[0])] = item\n",
    "\n",
    "    usr_file = usr_info_path\n",
    "    usr_info = {}\n",
    "    # 打开文件，读取所有行到data中\n",
    "    with open(usr_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            usr_info[str(item[0])] = item\n",
    "\n",
    "    \n",
    "   \n",
    "    watched_mov=[]\n",
    "    # 根据用户ID获得该用户看过的电影\n",
    "    usr_ID = usr_id\n",
    "    ratings_topk=gen_movie_with_usr(usr_ID,5)\n",
    "    for k, score in ratings_topk:\n",
    " #           print(\"电影ID: {}，评分是: {}, 电影信息: {}\".format(k, score, movie_info[k]))\n",
    "            watched_mov.append(k)\n",
    "\n",
    "    mov_id = np.random.choice(len(watched_mov), 1)[0]\n",
    "#拿这部电影作为推荐的基本       \n",
    "    based_mov=watched_mov[mov_id]\n",
    "\n",
    "    # 读取保存的电影特征\n",
    "    mov_feat_dir = mov_feat_dir\n",
    "    mov_feats = pickle.load(open(mov_feat_dir, 'rb'))\n",
    "    # 根据用户ID索引到该用户的特征\n",
    "\n",
    "\n",
    "\n",
    "    mov_query_feat = mov_feats[str(based_mov)]\n",
    "\n",
    "    # 记录计算的相似度\n",
    "    cos_sims = []\n",
    "    # 记录下与用户特征计算相似的电影顺序\n",
    "\n",
    "    with dygraph.guard():\n",
    "        # 索引电影特征，计算和输入用户ID的特征的相似度\n",
    "        for idx, key in enumerate(mov_feats.keys()):\n",
    "            mov_feat = mov_feats[key]\n",
    "            mov_query_feat = dygraph.to_variable(mov_query_feat)\n",
    "            mov_feat = dygraph.to_variable(mov_feat)\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            sim = fluid.layers.cos_sim(mov_query_feat,mov_feat)\n",
    "            # 打印特征和相似度的形状\n",
    "#            if idx==0:\n",
    "#                print(\"用户特征形状：{}, 用户特征形状：{}, 相似度结果形状：{}，相似度结果：{}\".format(usr_query_feat.shape, usr_feat0.shape, sim.numpy().shape, sim.numpy()))\n",
    "            # 从形状为（1，1）的相似度sim中获得相似度值sim.numpy()[0][0]，并添加到相似度列表cos_sims中\n",
    "            cos_sims.append(sim.numpy()[0][0])\n",
    "\n",
    "  \n",
    "    # 3. 对相似度排序，获得最大相似度在cos_sims中的位置\n",
    "    index = np.argsort(cos_sims)\n",
    "    # 打印相似度最大的前topk个位置\n",
    " \n",
    "    topk = top_k+1\n",
    "#   print(\"相似度最大的前{}个索引是{}\\n对应的相似度是：{}\\n\".format(topk-1, index[-topk:-1], [cos_sims[k] for k in index[-topk:-1]]))\n",
    "    print('用户{}评分高的电影是：{}'.format(usr_ID,mov_info[based_mov]))\n",
    "    recommend_mov=[]\n",
    "    for i in index[-topk:-1]:           \n",
    "        print(\"与其相似的电影分别是：{}\".format(mov_info[list(mov_feats.keys())[i]]))\n",
    "        recommend_mov.append(mov_info[list(mov_feats.keys())[i]])\n",
    "\n",
    "       \n",
    "\n",
    "# 加入随机选择因素，确保每次推荐的结果稍有差别\n",
    "#    while len(res) < pick_num:\n",
    "#        mov_id = np.random.choice(len(recommend_mov), 1)[0]\n",
    "#        if recommend_mov[mov_id] not in res:\n",
    "#            res.append(recommend_mov[mov_id])\n",
    "\n",
    "#    for id in res:\n",
    "#        print(\"要推荐的电影为 mov_id:\", id, mov_info[str(id)])\n",
    "usr_feat_dir='usr_feat.pkl'\n",
    "mov_feat_dir='mov_feat.pkl'\n",
    "mov_info_path=\"./ml-1m/movies.dat\"\n",
    "usr_info_path=\"./ml-1m/users.dat\"\n",
    "\n",
    "recommend_mov_item_based(1, 5, 5, usr_feat_dir, mov_feat_dir,usr_info_path, mov_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前的用户是：\n",
      "usr_id: 1\n",
      "推荐可能喜欢的电影是：\n",
      "个性化推荐部分\n",
      "mov_id: 1898 ['1898', 'Land Girls, The (1998)', 'Drama|War']\n",
      "mov_id: 1198 ['1198', 'Raiders of the Lost Ark (1981)', 'Action|Adventure']\n",
      "mov_id: 858 ['858', 'Godfather, The (1972)', 'Action|Crime|Drama']\n",
      "mov_id: 1136 ['1136', 'Monty Python and the Holy Grail (1974)', 'Comedy']\n",
      "mov_id: 50 ['50', 'Usual Suspects, The (1995)', 'Crime|Thriller']\n",
      "热门推荐部分\n",
      "要推荐的电影为 mov_id: 480 ['480', 'Jurassic Park (1993)', 'Action|Adventure|Sci-Fi']\n",
      "要推荐的电影为 mov_id: 1210 ['1210', 'Star Wars: Episode VI - Return of the Jedi (1983)', 'Action|Adventure|Romance|Sci-Fi|War']\n",
      "新品推荐部分\n",
      "要推荐的电影为 mov_id: 3946 ['3946', 'Get Carter (2000)', 'Action|Drama|Thriller']\n",
      "要推荐的电影为 mov_id: 3949 ['3949', 'Requiem for a Dream (2000)', 'Drama']\n",
      "要推荐的电影为 mov_id: 3950 ['3950', 'Tigerland (2000)', 'Drama']\n"
     ]
    }
   ],
   "source": [
    "#热门、新品和个性化推荐。三种各占比例2、3、5条\n",
    "#热门可以认为是被评分最多的电影，新品可以认为是上映时间最晚的电影，个性化推荐就按用户特征的embedding和电影特征的embedding相似度最高的就行\n",
    "def recommend_mov_for_usr_mix(usr_id,  pick_num, usr_feat_dir, mov_feat_dir, mov_info_path):\n",
    " #首先来看个性化推荐的部分\n",
    "\n",
    "    person_num=int(pick_num*0.5)\n",
    "    top_k=3*person_num\n",
    "\n",
    "\n",
    "    # 读取电影和用户的特征\n",
    "    usr_feats = pickle.load(open(usr_feat_dir, 'rb'))\n",
    "    mov_feats = pickle.load(open(mov_feat_dir, 'rb'))\n",
    "    usr_feat = usr_feats[str(usr_id)]\n",
    "\n",
    "    cos_sims = []\n",
    "\n",
    "    with dygraph.guard():\n",
    "        # 索引电影特征，计算和输入用户ID的特征的相似度\n",
    "        for idx, key in enumerate(mov_feats.keys()):\n",
    "            mov_feat = mov_feats[key]\n",
    "            usr_feat = dygraph.to_variable(usr_feat)\n",
    "            mov_feat = dygraph.to_variable(mov_feat)\n",
    "            sim = fluid.layers.cos_sim(usr_feat, mov_feat)\n",
    "            cos_sims.append(sim.numpy()[0][0])\n",
    "    # 对相似度排序\n",
    "    index = np.argsort(cos_sims)[-top_k:]\n",
    "\n",
    "    mov_info = {}\n",
    "    # 读取电影文件里的数据，根据电影ID索引到电影信息\n",
    "    with open(mov_info_path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        data = f.readlines()\n",
    "        for item in data:\n",
    "            item = item.strip().split(\"::\")\n",
    "            mov_info[str(item[0])] = item\n",
    "            \n",
    "    print(\"当前的用户是：\")\n",
    "    print(\"usr_id:\", usr_id)\n",
    "    print(\"推荐可能喜欢的电影是：\")\n",
    "    res = []\n",
    "    \n",
    "    # 加入随机选择因素，确保每次推荐的都不一样\n",
    "    while len(res) < person_num:\n",
    "        val = np.random.choice(len(index), 1)[0]\n",
    "        idx = index[val]\n",
    "        mov_id = list(mov_feats.keys())[idx]\n",
    "        if mov_id not in res:\n",
    "            res.append(mov_id)\n",
    "    print(\"个性化推荐部分\")\n",
    "    for id in res:\n",
    "        print(\"mov_id:\", id, mov_info[str(id)])\n",
    "#热门部分，被点评次数最多的电影\n",
    "    hot_num=int(pick_num*0.2)\n",
    "    hot_top_k=3*hot_num\n",
    "\n",
    "    rating_path = \"./ml-1m/ratings.dat\"\n",
    "    # 打开文件，ratings_data\n",
    "    with open(rating_path, 'r') as f:\n",
    "        ratings_data = f.readlines()\n",
    "    \n",
    "    mov_rating_count = {}\n",
    "    for item in ratings_data:\n",
    "        item = item.strip().split(\"::\")\n",
    "        # 处理每行数据，分别得到用户ID，电影ID，和评分\n",
    "        usr_id,movie_id,score = item[0],item[1],item[2]\n",
    "\n",
    "        if str(movie_id) in mov_rating_count:\n",
    "            mov_rating_count[movie_id] = mov_rating_count[movie_id]+1\n",
    "        else:\n",
    "            mov_rating_count[movie_id] =1\n",
    "\n",
    "    hot_topk = sorted(mov_rating_count.items(), key=lambda item:item[1])[-hot_top_k:]\n",
    "    hot_recommend_mov=[]\n",
    "    for k, score in hot_topk:\n",
    " #           print(\"电影ID: {}，评分次数是: {}, 电影信息: {}\".format(k, score, movie_info[k]))\n",
    "            hot_recommend_mov.append(k)\n",
    "#用于推荐的电影\n",
    "   \n",
    "    hot_res = []\n",
    "# 加入随机选择因素，确保每次推荐的结果稍有差别\n",
    "    while len(hot_res) < hot_num:\n",
    "        mov_id = np.random.choice(len(hot_recommend_mov), 1)[0]\n",
    "        if hot_recommend_mov[mov_id] not in res:\n",
    "            hot_res.append(hot_recommend_mov[mov_id])\n",
    "    print('热门推荐部分')\n",
    "    for id in hot_res:\n",
    "        print(\"要推荐的电影为 mov_id:\", id, mov_info[str(id)])\n",
    "#新品部分,选时间最晚上映的\n",
    "    new_num=int(pick_num*0.3)\n",
    "    new_top_k=3*new_num\n",
    "\n",
    "    mov_year={}\n",
    "    for index,info in mov_info.items():       \n",
    "        mov_year[index]=int(str(info[1])[-5:-1])\n",
    "    \n",
    "    new_topk = sorted(mov_year.items(), key=lambda item:item[1])[-new_top_k:]\n",
    "    new_recommend_mov=[]\n",
    "    for k, year in new_topk:\n",
    " #           print(\"电影ID: {}，评分次数是: {}, 电影信息: {}\".format(k, score, movie_info[k]))\n",
    "            new_recommend_mov.append(k)\n",
    "#用于推荐的电影\n",
    "   \n",
    "    new_res = []\n",
    "# 加入随机选择因素，确保每次推荐的结果稍有差别\n",
    "    while len(new_res) < new_num:\n",
    "        mov_id = np.random.choice(len(new_recommend_mov), 1)[0]\n",
    "        if new_recommend_mov[mov_id] not in res:\n",
    "            new_res.append(new_recommend_mov[mov_id])\n",
    "    print('新品推荐部分')\n",
    "    for id in new_res:\n",
    "        print(\"要推荐的电影为 mov_id:\", id, mov_info[str(id)])\n",
    "\n",
    "\n",
    "movie_data_path = \"./ml-1m/movies.dat\"\n",
    "\n",
    "pick_num = 10\n",
    "\n",
    "recommend_mov_for_usr_mix(1, pick_num, 'usr_feat.pkl', 'mov_feat.pkl', movie_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
